# Resume-Cloud-Challenge

  When starting off into the cloud computing atmosphere, I had little to no experience whatsoever, in terms of Python, HTML, JavaScript, let-alone, anything involving cloud computing in general. Following the Cloud Resume Challenge has definitely pulled me into new kinds of waters that, as is expected, challenges me to face all kinds of situations that seem to twist and turn into new paths of understanding and technical comprehension. To say the least, I would be more than happy to further pursue the skills I have learned through this learning process.

  When I first found out about Cloud computing and cloud services, it was when I was in high school and companies like Salesforce, Amazon, and Microsoft were becoming giant celebrities in the tech world for splurging big bucks into what was called Cloud. I thought it was some kind of new wizardry where we finally store giant servers in satellites, or space stations with gigantic computers stuffed into them, like a modern monolith of technology! It was more recently that I found out from my dad, an IT guru of all sorts, that cloud systems were just machines in someone else’s hands that we access. 

  Digging deeper, I talked to my cousin in Azure Cloud Services as a Cloud consultant for a Hitachi about what cloud services are and what he does for a living. Frankly, it wasn’t the clearest, but I learned that cloud services can be a potential god-send blessing to businesses that rely on security, speed, and budgeting in terms of data necessities. 

If you were just as lost as I was, let me explain a few basic services that are provided, more specifically, with AWS (Amazon Web Services).

  With the curiosity in my heart, and the ambition in my mind, I reluctantly dredged through all the ups and downs that come with creating a web-resume hosted on the cloud. Starting off was pretty easy, I first started by setting up my root account for AWS, then proceeded to make a secondary account as an IAM admin user which stands for Identity Access Management. This allows me to monitor and set up certain kinds of security to ensure that nothing can be accessed or tampered with unless it was me (and I tampered a lot.) 

  Then I played with things such as EC2 which stands for Elastic Compute Cloud. This was a fascinating service as it showed how cloud systems could be used for various ranges of computing power for data processing and other purposes you may find. 

  After enough poking around, I started on my resume platform, taking a moment to set up all the files like your code or any images you may want to refer to within the website page. For me, I created what is known as a bucket, which is AWS’ fancy form of a file or folder as part of its S3 (Simple Storage Service), where I stored my HTML, CSS, and JavaScript.

  Then I set up a MySQL database for all my collected data from the website, using AWS’ RDS, standing for Relational Database Service. With this, I additionally used DynamoDB as a NoSQL database service which would managed the database and make sure it keeps up to date as fast as possible with no run-time errors. These initial setups pave a way for me to create a JavaScript for a visitor counter on my website to add one more to the counter whenever someone looks at my page. 

  And after some very creative naming for the domain, I purchased a domain from namecheap.com and took a look at the kinds of services AWS provides. First, I used AWS’ Route 53 to set up hosted zones for my domain to route traffic appropriately so that it does not overwhelm the services nor redirect the users somewhere incorrectly. Following this step, I made extra sure to create a proper SSL certificate through AWS Certificate Manager. This would register my domain to be secured under HTTPS instead of the non-secured HTTP. 

  With the SSL certificate ensured, I was able to move on to setting up invalidations with CloudFront which creates an extra step, at least for me, to reduce redundancy and allows me to clear any unwanted cache before it automatically expires. This has been useful for every time I update the website code, so that the live site resets its cache and shows the new output.

  Finally, solidified the connections between my JavaScript and the API Gateway using Lambda to provide serverless code. Since this was more gibberish, it’s best to note that this is a necessary step because it is what allows the visitor counter that was mentioned before to work. The API in this hand-off allows for the code and the database to communicate with the website since the DynamoDB does not communicate directly with code.

  With all that listed, I would like to lastly mention a few remarkable experiences that I faced along the way. The biggest issues of them all. The number one time killer were bugs in my code. Not necessarily the processes in the AWS toolbox. My biggest issues was when my JavaScript, in multiple varying files, would have the tiniest syntax errors. If I would have gone through this whole project again, I would definitely have taken my time slower to see the small, yet important, details that break the rest of the machine. The second time killer, organization. When you have a million things running in the cloud, really take a moment through every step to evaluate what is necessary to keep and what isn’t along the way. There were many times where I wasted time and money on data that was circulating around unnecessarily. On top of this, I would delete the wrong instance or file in my S3 buckets. Best word of advice I could give, write down all your steps in order so you don’t lose track of responsibilities, and keep all tasks to a minimum for detail oriented progress. 


